{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B тест для крупного интернет-магазина\n",
    "\n",
    "Нам предстоит проанализировать данные для крупного интернет-магазина. Вместе с отделом маркетинга мы разработали список гипотез для увеличения выручки. Необходимо <b>приоритизировать гипотезы</b>, запустить <b>A/B тест</b>, <b>проанализировать результаты</b> и <b>принять решение</b> о дальнейших действиях на основе проведенного анализа. Разделим данные исследование на несколько шагов:\n",
    "\n",
    "## <a href='#section0'>0. Загрузка библиотек</a><br>\n",
    "## <a href='#section1'>1. Приоритизация гипотез</a><br>\n",
    "### <a href='#section10'>1.0 Импорт и предобработка файла</a><br>\n",
    "### <a href='#section11'>1.1 Фреймворк ICE</a><br>\n",
    "### <a href='#section12'>1.2 Фреймворк RICE</a><br>\n",
    "## <a href='#section2'>2. Анализ A/B теста</a><br>\n",
    "### <a href='#section20'>2.0. Импорт и предобработка файлов</a><br>\n",
    "### <a href='#section21'>2.1. Кумулятивная выручка по группам</a><br>\n",
    "### <a href='#section22'>2.2. Кумулятивный средний чек по группам</a><br>\n",
    "### <a href='#section23'>2.3. Кумулятивная конверсия по группам</a><br>\n",
    "### <a href='#section24'>2.4. Выбор границы для определения аномальных пользователей</a><br>\n",
    "### <a href='#section25'>2.5. Выбор границы для определения аномальных заказов</a><br>\n",
    "### <a href='#section26'>2.6. Различия в конверсии между группами по «сырым» данным</a><br>\n",
    "### <a href='#section27'>2.7. Различия в среднем чеке заказа между группами по «сырым» данным</a><br>\n",
    "### <a href='#section28'>2.8. Различия в конверсии между группами по «очищенным» данным</a><br>\n",
    "### <a href='#section29'>2.9. Различия в среднем чеке заказа между группами по «очищенным» данным</a><br>\n",
    "## <a href='#section3'>3. Принятие решения по итогам A/B теста</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section0'>0. Загрузка библиотек</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеки\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#Выберем стили графиков\n",
    "large = 16; med = 12; small = 10\n",
    "params = {'axes.titlesize': med,\n",
    "          'axes.titleweight': 'light',\n",
    "          'axes.labelweight': 'light',\n",
    "          'legend.fontsize': small,\n",
    "          'figure.figsize': (12, 6),\n",
    "          'axes.labelsize': med,\n",
    "          'axes.titlesize': large,\n",
    "          'xtick.labelsize': small,\n",
    "          'ytick.labelsize': small,\n",
    "          'figure.titlesize': large}\n",
    "plt.rcParams.update(params)\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section1'>1. Приоритизация гипотез</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section10'>1.0 Импорт и предобработка файла</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /datasets/hypothesis.csv does not exist: '/datasets/hypothesis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efba126b1757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Импортируем файл с гипотезами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/datasets/hypothesis.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Назначаем имя датафрейма\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'hypothesis'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Проверим данные на пропуски и аномалии\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File /datasets/hypothesis.csv does not exist: '/datasets/hypothesis.csv'"
     ]
    }
   ],
   "source": [
    "#Импортируем файл с гипотезами\n",
    "hypothesis = pd.read_csv('/datasets/hypothesis.csv')\n",
    "#Назначаем имя датафрейма\n",
    "hypothesis.name = 'hypothesis'\n",
    "#Проверим данные на пропуски и аномалии\n",
    "def check_dataframe(dataframe):\n",
    "    print('Проверяем датафрейм:', dataframe.name)\n",
    "    display(dataframe.head(10))\n",
    "    display(dataframe.describe())\n",
    "    display(dataframe.info())\n",
    "    display(dataframe.duplicated().unique())\n",
    "check_dataframe(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных нет явных пропусков, ошибок, аномалий и дубликатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Настроим датафрейм так, чтобы гипотезы отобажались полностью\n",
    "pd.set_option('max_colwidth', 400)\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section11'>1.1 Фреймворк ICE</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Находим значение ICE Score по формуле Impact * Confidence / Efforts и отсортируем имеющиеся гипотезы\n",
    "hypothesis['ICE_Score'] = hypothesis['Impact'] * hypothesis['Confidence'] / hypothesis['Efforts']\n",
    "#Нарисуем график оценки гипотез по ICE Score\n",
    "x = pd.Series(range(0, len(hypothesis['ICE_Score'])))\n",
    "y = hypothesis['ICE_Score'].values\n",
    "types =  hypothesis.index\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel('Порядковый номер гипотезы')\n",
    "ax.set_ylabel('ICE Score')\n",
    "ax.set_title('Оценка гипотез по ICE Score')\n",
    "ax.set_xticklabels([])\n",
    "for i, txt in enumerate(types):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(10,10), textcoords='offset points')\n",
    "    plt.scatter(x, y, marker=\"d\", color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат - у гипотезы 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Гипотеза 8:', hypothesis.iloc[8,0])\n",
    "print('Оценка ICE Score:', hypothesis.iloc[8,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно фреймворку ICE, в первую очередь подлежит проверки Гипотеза № 8: \"Запустить акцию, дающую скидку на товар в день рождения\" с оценкой ICE Score 16.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section12'>1.2 Фреймворк RICE</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Находим значение RICE Score по формуле Reach * Impact * Confidence / Efforts и отсортируем имеющиеся гипотезы\n",
    "hypothesis['RICE_Score'] = (hypothesis['Reach'] * hypothesis['Impact'] * hypothesis['Confidence'])/ hypothesis['Efforts']\n",
    "#Нарисуем график оценки гипотез по RICE Score\n",
    "x = pd.Series(range(0, len(hypothesis['RICE_Score'])))\n",
    "y = hypothesis['RICE_Score'].values\n",
    "types =  hypothesis.index\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel('Порядковый номер гипотезы')\n",
    "ax.set_ylabel('RICE Score')\n",
    "ax.set_title('Оценка гипотез по RICE Score')\n",
    "ax.set_xticklabels([])\n",
    "for i, txt in enumerate(types):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(10,10), textcoords='offset points')\n",
    "    plt.scatter(x, y, marker=\"s\", color='mediumblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучший результат - у гипотезы 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Гипотеза 7:', hypothesis.iloc[7,0])\n",
    "print('Оценка RICE Score:', hypothesis.iloc[7,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно фреймворку RICE, в первую очередь подлежит проверке Гипотеза № 7: \"Добавить форму подписки на все основные страницы, чтобы собрать базу клиентов для email-рассылок\" с оценкой RICE Score 112."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посмотрим, как распределятся оценки гипотез по ICE/RICE фреймворкам на одном графике\n",
    "x = hypothesis['ICE_Score'].values\n",
    "y = hypothesis['RICE_Score'].values\n",
    "types =  hypothesis.index\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel('ICE Score')\n",
    "ax.set_ylabel('RICE Score')\n",
    "ax.set_title('Распределение оценок гипотез по ICE/RICE фреймворкам')\n",
    "for i, txt in enumerate(types):\n",
    "    ax.annotate(txt, (x[i], y[i]), xytext=(10,10), textcoords='offset points')\n",
    "    plt.scatter(x, y, marker=\"D\", color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Появился еще один лидер: оценка гипотезы 0 превосходит оценку гипотезы 7 по фреймворку ICE. Но, как и гипотеза 8, уступает в фреймворке RICE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Гипотеза 0:', hypothesis.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод\n",
    "Фреймворк RICE в отличие от ICE учитывает в т.ч. охват пользователей для теста. Для нас он будет приоритетным. Так, скидка на товар в день рождения - это очень ограниченная акция. Проверка такой гипотезы может потребовать более длительного времени, поскольку в среднем ежедневно только 1/365 от всех пользователей могут участвовать в этом тесте. Если учесть, что не все пользователи увидят сообщения об акции, а те которые увидят и примут участие будут разделены на контрольную и тестовую группу, то пользователей будет еще меньше. Если же тест будет слишком продолжительным, на него могут повлиять макроэкономические факторы, сезонность и действия конкурентов. Для проведения нашего A/B теста остановимся на гипотезе с максимальным RICE Score: Гипотеза № 7 \"Добавить форму подписки на все основные страницы, чтобы собрать базу клиентов для email-рассылок\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section2'>2. Анализ A/B теста</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section20'>2.0. Импорт и предобработка файлов</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем файлы\n",
    "orders = pd.read_csv('/datasets/orders.csv')\n",
    "visitors = pd.read_csv('/datasets/visitors.csv')\n",
    "#Назначаем имена датафреймов\n",
    "orders.name = 'orders'\n",
    "visitors.name = 'visitors'\n",
    "#Проверяем данные на пропуски и аномалии\n",
    "check_dataframe(orders)\n",
    "check_dataframe(visitors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных нет явных пропусков, ошибок, аномалий и дубликатов. Столбцы с датами времени необходимо перевести в соответсвующий формат. Там где это возможно, изменим тип данных для оптимизации размера файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Оптимизируем типы данных\n",
    "convert_dict_orders = {'transactionId': 'uint32', 'visitorId':'uint32', 'revenue':'uint32'}\n",
    "orders = orders.astype(convert_dict_orders)\n",
    "visitors['visitors'] = visitors['visitors'].astype('uint32')\n",
    "#Переводим стобцы с датами в нужный формат\n",
    "visitors['date'] = visitors['date'].map(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
    "orders['date'] = orders['date'].map(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Названия столбцов и типы данных изменены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверим, не пересекаются ли пользователи в группах\n",
    "uniqueVisitorsA = orders[orders['group'] == 'A']['visitorId'].unique()\n",
    "uniqueVisitorsB = orders[orders['group'] == 'B']['visitorId'].unique()\n",
    "visitorsIntersection = np.intersect1d(uniqueVisitorsA,uniqueVisitorsB)\n",
    "visitorsIntersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visitorsIntersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58 уникальных пользователей находятся и в группе А, и в группе B. Это означает, что в ходе теста они \"перетекли\" из одной группы в другую, внося шум в итоговые результаты. Исключим их сразу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders[np.logical_not(orders['visitorId'].isin(visitorsIntersection))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section21'>2.1. Кумулятивная выручка по группам</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим массив из уникальных пар значений дат и групп A/B теста\n",
    "datesGroups = orders[['date', 'group']].drop_duplicates()\n",
    "#Агрегируем таблицу заказов, оставив соответствующие datesGroups значения даты и группы, и посчитаем  количество уникальных посетителей сделавших заказы, количество транзакций и сумму выручки; затем отсортируем по дате и группе\n",
    "ordersAggregated = datesGroups.apply(lambda x: \\\n",
    "                                     orders[np.logical_and(orders['date'] <= x['date'], \\\n",
    "                                                           orders['group'] == x['group'])]\\\n",
    "                                     .agg({\n",
    "                                        'date':'max', \n",
    "                                        'group':'max', \n",
    "                                        'transactionId': pd.Series.nunique, \n",
    "                                        'visitorId': pd.Series.nunique, \n",
    "                                        'revenue':'sum'}), axis=1).sort_values(by=['date', 'group'])\n",
    "ordersAggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Агрегируем таблицу посетителей, оставив соответствующие datesGroups значения даты и группы, и посчитаем количество уникальных посетителей; затем отсортируем по дате и группе\n",
    "visitorsAggregated = datesGroups.apply(lambda x: visitors[np.logical_and(visitors['date'] <= x['date'], \\\n",
    "                                                                         visitors['group'] == x['group'])]\\\n",
    "                                       .agg({\n",
    "                                            'date': 'max', \n",
    "                                            'group':'max', \n",
    "                                            'visitors':'sum'}), axis=1).sort_values(by=['date', 'group'])\n",
    "visitorsAggregated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Объединим две полученные таблицы в одну и изменим названия столбцов\n",
    "cumulativeData = ordersAggregated.merge(visitorsAggregated, left_on=['date', 'group'], right_on=['date', 'group'])\n",
    "cumulativeData.columns = ['date', 'group', 'transactions', 'buyers', 'revenue', 'visitors']\n",
    "cumulativeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим датафреймы с кумулятивным количеством заказов и кумулятивной выручкой для каждой из групп\n",
    "cumulativeRevenueA = cumulativeData[cumulativeData['group'] == 'A'][['date', 'revenue', 'transactions']]\n",
    "cumulativeRevenueB = cumulativeData[cumulativeData['group'] == 'B'][['date', 'revenue', 'transactions']]\n",
    "#Строим графики кумулятивной выручки для каждой из групп\n",
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue'], label='A')\n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue'], label='B')\n",
    "#Добавим подписи к графикам\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Кумулятивная выручка')\n",
    "plt.title('Кумулятивная выручка по группам')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выручка равномерно увеличивается в течение теста, но после 17 августа в группе B произошел либо всплеск числа заказов, либо была произведена аномально дорогие заказы. Данные выбросы необходимо проанализировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section22'>2.2. Кумулятивный средний чек по группам</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построим графики кумулятивного среднего чека по группам, средний чек найдем через отношение всей выручки к числу заказов\n",
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue']/cumulativeRevenueA['transactions'], label='A')\n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue']/cumulativeRevenueB['transactions'], label='B')\n",
    "#Добавим подписи к графикам\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Кумулятивный средний чек')\n",
    "plt.title('Кумулятивный средний чек по группам')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средний чек для обеих групп выравнивался до середины теста, затем средний чек в группе B становится значительно больше среднего чека в группе A. Возможно, резкий всплеск синего графика говорит о том, что в группу B попали крупные заказы. Это сместило реальный средний чек для данной группы выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для определения относительного изменения кумулятивного среднего чека, соберем данные в одной таблице\n",
    "mergedCumulativeRevenue = cumulativeRevenueA.merge(cumulativeRevenueB, \\\n",
    "                                                   left_on='date', \n",
    "                                                   right_on='date', \n",
    "                                                   how='left', \n",
    "                                                   suffixes=['A', 'B'])\n",
    "#Построим график относительного изменения кумулятивного среднего чека группы B к группе A\n",
    "plt.plot(mergedCumulativeRevenue['date'], \\\n",
    "        (mergedCumulativeRevenue['revenueB']/mergedCumulativeRevenue['transactionsB'])\\\n",
    "         /(mergedCumulativeRevenue['revenueA']/mergedCumulativeRevenue['transactionsA']) - 1)\n",
    "#Добавляем ось X\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Отношение кумулятивного среднего чека')\n",
    "plt.title('Отношение кумулятивного среднего чека группы B к группе A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нескольких точках график различия резко меняется. Вероятно, причиной этого являются крупные заказы и выбросы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section23'>2.3. Кумулятивная конверсия по группам</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посчитаем кумулятивную конверсию\n",
    "cumulativeData['conversion'] = cumulativeData['transactions']/cumulativeData['visitors']\n",
    "#Выделим данные для обеих групп в отдельные таблицы\n",
    "cumulativeDataA = cumulativeData[cumulativeData['group'] == 'A']\n",
    "cumulativeDataB = cumulativeData[cumulativeData['group'] == 'B']\n",
    "#Построим графики\n",
    "plt.plot(cumulativeDataA['date'], cumulativeDataA['conversion'], label='A')\n",
    "plt.plot(cumulativeDataB['date'], cumulativeDataB['conversion'], label='B')\n",
    "plt.legend()\n",
    "#Задаем масштаб осей\n",
    "plt.axis(['2019-08-01', '2019-08-31', 0.01, 0.05])\n",
    "#Добавим подписи к графикам\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Кумулятивная конверсия')\n",
    "plt.title('Кумулятивная конверсия по группам')\n",
    "#Добавляем ось X\n",
    "plt.axhline(y=0.028, color='grey', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале теста группы колебались в районе значения 0.028, но затем конверсия группы B стала больше и зафиксировалась на уровне 0.029, а конверсия группы A просела и также зафиксировалась на уровне 0.026. Кривые стабилизировались в конце теста и это косвенный признак возможности скорого завершения теста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Объединим данные о кумулятивных конверсиях в одну таблицу\n",
    "mergedCumulativeConversions = cumulativeDataA[['date', 'conversion']].merge(cumulativeDataB[['date', 'conversion']], \\\n",
    "                                                                            left_on='date', \n",
    "                                                                            right_on='date', \n",
    "                                                                            how='left', \n",
    "                                                                            suffixes=['A', 'B'])\n",
    "#Построим график относительного различия кумулятивных конверсий\n",
    "plt.plot(mergedCumulativeConversions['date'], \\\n",
    "         mergedCumulativeConversions['conversionB']/\\\n",
    "         mergedCumulativeConversions['conversionA'] - 1)\n",
    "#Задаем масштаб осей \n",
    "plt.axis(['2019-08-01', '2019-08-31', -0.3, 0.3])\n",
    "#Добавляем оси X\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "plt.axhline(y=0.1, color='darkgrey', linestyle='--')\n",
    "plt.axhline(y=0.2, color='darkgrey', linestyle='--')\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Дата')\n",
    "plt.ylabel('Отношение кумулятивной конверсии')\n",
    "plt.title('Отношение кумулятивной конверсии группы B к группе A')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале теста отношение конверсий не установилось, но затем конверсия группы B стала превышать конверсию группы A и держится в коридоре прироста 12-20% относительно группы A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section24'>2.4. Выбор границы для определения аномальных пользователей</a><br></a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сгруппируем таблицу заказов по пользователям\n",
    "ordersByBuyers = orders.drop(['date', 'revenue'], axis=1).groupby(['group', 'visitorId'], as_index=False)\\\n",
    "                .agg({'transactionId': pd.Series.nunique}).sort_values(by='transactionId', ascending=False)\n",
    "ordersByBuyers.columns = ['group', 'buyerId', 'transactions']\n",
    "#Изучим гистограмму распределения\n",
    "plt.hist(ordersByBuyers['transactions'])\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Количество заказов')\n",
    "plt.ylabel('Количество уникальных пользователей')\n",
    "plt.title('Распределение количества заказов на  одного пользователя')\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство посетителей покупают только 1 раз и есть небольшая группа которые покупают чаще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построим точечную диаграмму числа заказов на одного посетителя\n",
    "x_values1 = pd.Series(range(0, len(ordersByBuyers['transactions'])))\n",
    "sns.scatterplot(x=x_values1, y='transactions', data=ordersByBuyers, alpha=0.7, y_jitter=0.05, hue='group')\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Количество уникальных пользователей')\n",
    "plt.ylabel('Количество заказов')\n",
    "plt.title('Распределение количества заказов на  одного пользователя по группам')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы прошлого графика подтвердились: в основном, посетители делают 1 заказ. Посетители, которые делают более 2 заказов распределены между группами хаотично и нет какой-то определенной тенденции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(ordersByBuyers['transactions'], [90, 95, 99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% пользователей делают не более 1 заказа. Установим это за нижнюю границу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section25'>2.5. Выбор границы для определения аномальных заказов</a><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Изучим гистограмму распределения стоимостей заказов\n",
    "plt.hist(orders['revenue'], bins=500)\n",
    "plt.xlim(0, 100000)\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Выручка с заказов')\n",
    "plt.ylabel('Количество уникальных пользователей')\n",
    "plt.title('Распределение стоимостей заказов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная масса транзакций не превышает 20 000 рублей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построим точечную диаграмму стоимостей заказов\n",
    "x_values2 = pd.Series(range(0, len(orders['revenue'])))\n",
    "sns.scatterplot(x=x_values2, y='revenue', data=orders, alpha=0.4, hue='group')\n",
    "plt.ylim(0, 100000)\n",
    "#Добавим подписи к графику\n",
    "plt.xlabel('Количество уникальных пользователей')\n",
    "plt.ylabel('Выручка с заказов')\n",
    "plt.title('Распределение стоимостей заказов по группам')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построим точечные диаграммы стоимостей заказов по группам с ограничением выручки 20000 рублей\n",
    "plt.subplots(figsize=(15,6))\n",
    "#График для группы 'A'\n",
    "x = pd.Series(range(0, len(orders[orders['group'] == 'A']['revenue'])))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x=x, y='revenue', data=orders[orders['group'] == 'A'], alpha=0.7, color='#fc8a23')\n",
    "plt.ylim(0, 20000)\n",
    "plt.xticks([])\n",
    "plt.ylabel('Выручка с заказов', fontsize=12)\n",
    "plt.title('Распределение стоимостей заказов для группы A', fontsize=14)\n",
    "#График для группы 'B'\n",
    "x = pd.Series(range(0, len(orders[orders['group'] == 'B']['revenue'])))\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x=x, y='revenue', data=orders[orders['group'] == 'B'], alpha=0.7)\n",
    "plt.ylim(0, 20000)\n",
    "plt.xticks([])\n",
    "plt.ylabel('Выручка с заказов', fontsize=12)\n",
    "plt.title('Распределение стоимостей заказов для группы B', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, заказов дороже 20000 рублей не так много. Эта граница, которую мы определим для аномальных заказов по графику. На первый взгляд, заказы распределены хаотично между группами и нет какой-то определенной тенденции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Посчитаем перцентили для определения более точной границы\n",
    "print(np.percentile(orders['revenue'], [90, 95, 99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не более 5% заказов имеют стоимость выше 26785 рублей. Возьмем это за нижнюю границу стоимости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section26'>2.6. Различия в конверсии между группами по «сырым» данным</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Нулевая гипотеза 1 - Статистически значимых различий в конверсии между группами по \"сырым\" данным нет. Альтернативная гипотеза 1 - Статистически значимые различия в конверсии между группами по \"сырым\" данным есть. Уровень статистической значимости  p < 0.05</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделим всех посетителей по группам и датам\n",
    "visitorsADaily = visitors[visitors['group'] == 'A'][['date', 'visitors']]\n",
    "visitorsADaily.columns = ['date', 'visitorsPerDateA']\n",
    "visitorsBDaily = visitors[visitors['group'] == 'B'][['date', 'visitors']]\n",
    "visitorsBDaily.columns = ['date', 'visitorsPerDateB']\n",
    "#Найдем количество заказов по группам и датам\n",
    "ordersADaily = orders[orders['group'] == 'A'][['date', 'transactionId', 'revenue']]\\\n",
    "                    .groupby('date', as_index=False)\\\n",
    "                    .agg({'transactionId': pd.Series.nunique, 'revenue': 'sum'})\n",
    "ordersADaily.columns = ['date', 'ordersPerDateA', 'revenuePerDateA']\n",
    "ordersBDaily = orders[orders['group'] == 'B'][['date', 'transactionId', 'revenue']]\\\n",
    "                    .groupby('date', as_index=False)\\\n",
    "                    .agg({'transactionId': pd.Series.nunique, 'revenue': 'sum'})\n",
    "ordersBDaily.columns = ['date', 'ordersPerDateB', 'revenuePerDateB']\n",
    "#Соединим полученные четыре таблицы в одну\n",
    "daily_data = ordersADaily.merge(ordersBDaily,\\\n",
    "                                left_on='date', \n",
    "                                right_on='date', \n",
    "                                how='left').merge(visitorsADaily,\\\n",
    "                                                left_on='date', \n",
    "                                                right_on='date', \n",
    "                                                how='left').merge(visitorsBDaily, \\\n",
    "                                                                 left_on='date', \n",
    "                                                                 right_on='date', \n",
    "                                                                 how='left')\n",
    "#Найдем количество заказов, сделанных пользователями разных групп\n",
    "ordersByUsersA = orders[orders['group'] == 'A']\\\n",
    "            .groupby('visitorId', as_index=False)\\\n",
    "            .agg({'transactionId': pd.Series.nunique})\n",
    "ordersByUsersA.columns = ['visitorId', 'orders']\n",
    "ordersByUsersB = orders[orders['group'] == 'B']\\\n",
    "            .groupby('visitorId', as_index=False)\\\n",
    "            .agg({'transactionId': pd.Series.nunique})\n",
    "ordersByUsersB.columns = ['visitorId', 'orders']\n",
    "#Для каждой из групп сохраним выборку, где каждый элемент число заказов определенного пользователя, в т.ч. ноль. Нули найдем разницей между суммой посетителей и количеством записей о заказах.\n",
    "sampleA = pd.concat([ordersByUsersA['orders'], pd.Series(0, index=\\\n",
    "    np.arange(daily_data['visitorsPerDateA'].sum() - len(ordersByUsersA['orders'])), name='orders')], axis=0)\n",
    "sampleB = pd.concat([ordersByUsersB['orders'], pd.Series(0, index=\\\n",
    "    np.arange(daily_data['visitorsPerDateB'].sum() - len(ordersByUsersB['orders'])), name='orders')], axis=0)\n",
    "#Определим, есть ли статистически значимые различия в конверсии между группами. \n",
    "alpha = 0.05\n",
    "result_raw_1 = st.mannwhitneyu(sampleA, sampleB)[1]\n",
    "print('p-значение: ', '{0:.3f}'.format(result_raw_1))\n",
    "if result_raw_1 < alpha:\n",
    "    print('Отвергаем нулевую гипотезу')\n",
    "else:\n",
    "    print('Не получилось отвергнуть нулевую гипотезу')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"сырых данных\" показал, что в конверсии между группами есть статистически значимые различия и мы отвергаем нулевую гипотезу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сравним прирост конверсии группы B к конверсии группы A через отношение средних в выборках\n",
    "print('{0:.2%}'.format(sampleB.mean()/sampleA.mean() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"сырых данных\" показал, что относительный прирост конверсии группы B к конверсии группы A равен 15.98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section27'>2.7. Различия в среднем чеке заказа между группами по «сырым» данным</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Нулевая гипотеза 2 - Статистически значимых различий в средних чеках заказа между группами по \"сырым\" данным нет. Альтернативная гипотеза 2 - Статистически значимые различия  в средних чеках заказа между группами по \"сырым\" данным есть. Уровень статистической значимости  p < 0.05</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Определим, есть ли статистически значимые различия в средних чеках между группами. Нулевая гипотеза - средние чеки равны. Альтернативная гипотеза - средние чеки не равны.\n",
    "alpha = 0.05\n",
    "result_raw_2 = st.mannwhitneyu(orders[orders['group'] == 'A']['revenue'], orders[orders['group'] == 'B']['revenue'])[1]\n",
    "print('p-значение: ', '{0:.3f}'.format(result_raw_2))\n",
    "if result_raw_2 < alpha:\n",
    "    print('Отвергаем нулевую гипотезу')\n",
    "else:\n",
    "    print('Не получилось отвергнуть нулевую гипотезу')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"сырых данных\" показал, что в средних чеках между группами нет статистически значимых различий и мы не можем отвергнуть нулевую гипотезу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определим относительное различие среднего чека\n",
    "print('{0:.2%}'.format(orders[orders['group'] == 'B']['revenue'].mean()/orders[orders['group'] == 'A']['revenue'].mean() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"сырых данных\" показал, что относительный прирост среднего чека группы B к среднему чеку группы A равен 28.66%. Разница значительная, посмотрим какой результат покажут \"очищенные данные\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section28'>2.8. Различия в конверсии между группами по «очищенным» данным</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Нулевая гипотеза 3 - Статистически значимых различий в конверсии между группами по \"очищенным\" данным нет. Альтернативная гипотеза 3 - Статистически значимые различия в конверсии между группами по \"очищенным\" данным есть. Уровень статистической значимости  p < 0.05</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найдем посетителей с большим количеством заказов\n",
    "VisitorsWithManyOrders = pd.concat([ordersByUsersA[ordersByUsersA['orders'] > 1]['visitorId'],\\\n",
    "                                    ordersByUsersB[ordersByUsersB['orders'] > 1]['visitorId']], axis=0)\n",
    "#Найдем посетителей с дорогими заказами\n",
    "VisitorsWithExpensiveOrders = orders[orders['revenue'] > 26785]['visitorId']\n",
    "#Соединим таблицы и удалим дубликаты\n",
    "abnormalVisitors = pd.concat([VisitorsWithManyOrders, VisitorsWithExpensiveOrders], axis=0)\\\n",
    "                                    .drop_duplicates()\\\n",
    "                                    .sort_values()\n",
    "print(abnormalVisitors.head(5))\n",
    "print(abnormalVisitors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили 86 аномальных пользователя. Исключим их действия, создав отфильтрованные выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создадим отфильтрованные выборки\n",
    "sampleAFiltered = pd.concat([ordersByUsersA[np.logical_not(ordersByUsersA['visitorId']\\\n",
    "                            .isin(abnormalVisitors))]['orders'],pd.Series(0, index=\\\n",
    "                            np.arange(daily_data['visitorsPerDateA'].sum() - len(ordersByUsersA['orders'])),\\\n",
    "                                                                                    name='orders')], axis=0)\n",
    "sampleBFiltered = pd.concat([ordersByUsersB[np.logical_not(ordersByUsersB['visitorId']\\\n",
    "                            .isin(abnormalVisitors))]['orders'],pd.Series(0, index=\\\n",
    "                            np.arange(daily_data['visitorsPerDateB'].sum() - len(ordersByUsersB['orders'])),\\\n",
    "                                                                                    name='orders')], axis=0)\n",
    "#Определим, есть ли статистически значимые различия в конверсии между группами.  Нулевая гипотеза - конверсии равны. Альтернативная гипотеза - конверсии не равны.\n",
    "alpha = 0.05\n",
    "result_clean_1 = st.mannwhitneyu(sampleAFiltered, sampleBFiltered)[1]\n",
    "print('p-значение: ', '{0:.3f}'.format(result_clean_1))\n",
    "if result_clean_1 < alpha:\n",
    "    print('Отвергаем нулевую гипотезу')\n",
    "else:\n",
    "    print('Не получилось отвергнуть нулевую гипотезу')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-значение немного увеличилось. Анализ \"очищенных данных\" показал, что в конверсии между группами есть статистически значимые различия и мы отвергаем нулевую гипотезу. Как и в случае с \"сырыми данными\", статистическая значимость достигнута. Это означает, что сегмент B значительно лучше сегмента A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сравним прирост конверсии группы B к конверсии группы A через отношение средних в выборках\n",
    "print('{0:.2%}'.format(sampleBFiltered.mean()/sampleAFiltered.mean() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"очищенных данных\" показал, что относительный прирост конверсии группы B к конверсии группы A равен 17.39%. Этот результат почти на 9% больше в относительном выражении, чем по итогам анализа \"сырых данных\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='section29'>2.9. Различия в среднем чеке заказа между группами по «очищенным» данным</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Нулевая гипотеза 4 - Статистически значимых различий в средних чеках заказа между группами по \"очищенным\" данным нет. Альтернативная гипотеза 4 - Статистически значимые различия  в средних чеках заказа между группами по \"очищенным\" данным есть. Уровень статистической значимости  p < 0.05</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Определим, есть ли статистически значимые различия в средних чеках между группами. Нулевая гипотеза - средние чеки равны. Альтернативная гипотеза - средние чеки не равны.\n",
    "alpha = 0.05\n",
    "result_clean_2 = st.mannwhitneyu(orders[np.logical_and(\n",
    "                                    orders['group'] == 'A',\n",
    "                                    np.logical_not(orders['visitorId'].isin(abnormalVisitors)))]['revenue'],\n",
    "                                 orders[np.logical_and(\n",
    "                                    orders['group'] == 'B',\n",
    "                                    np.logical_not(orders['visitorId'].isin(abnormalVisitors)))]['revenue'])[1]\n",
    "print('p-значение: ', '{0:.3f}'.format(result_clean_2))\n",
    "if result_clean_2 < alpha:\n",
    "    print('Отвергаем нулевую гипотезу')\n",
    "else:\n",
    "    print('Не получилось отвергнуть нулевую гипотезу')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"очищенных данных\" показал, что статистическая значимость различий среднего чека заказа не достигнута. Это означает, что удаление выбросов не помогло."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Определим относительное различие среднего чека\n",
    "print('{0:.2%}'.format(orders[np.logical_and(orders['group']=='B',\\\n",
    "                              np.logical_not(orders['visitorId'].isin(abnormalVisitors)))]['revenue'].mean()/\\\n",
    "                       orders[np.logical_and(orders['group']=='A',\\\n",
    "                              np.logical_not(orders['visitorId'].isin(abnormalVisitors)))]['revenue'].mean() - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ \"очищенных данных\" показал, что относительный прирост конверсии группы B к конверсии группы A стал отрицателен и равен -3.37%. Это означает, что средний чек группы А в конце немного вырвался вперед, но расхождение очень незначительное."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section3'>3. Принятие решения по итогам A/B теста</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Имеются статистически значимые различия конверсии между группами и по \"сырым\" и по \"очищенным данным. Относительный прирост конверсии группы B к конверсии группы A равен 17.39% - это хороший результат\n",
    "2. Статистическая значимость различия среднего чека заказов не достигнута ни по \"сырым\", ни по \"очищенным данным\". Более того, после фильтрации аномалий отношение средних чеков стало совсем незначительным (3.37% в пользу группы 'A')\n",
    "\n",
    "Это может означать, что посетители из группы 'B' более замотивированы к покупке, но по какой-то причине они приносят чуть меньше выручки, чем посетители из группы 'A'. То есть увеличив конверсию, мы, возможно, понизили доходность. Вероятно, проблема в рекламном сообщении / источнике трафика/ таргетинге/ демографических характеристиках посетителей / качестве выборки или других причинах. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остановить тест и признать его неуспешным. На основе базовой гипотезы создать несколько новых тестов, в которых будет исследовано влияние дополнительных факторов на базовую гипотезу."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
